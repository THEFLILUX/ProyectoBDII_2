{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de Datos II - Proyecto II: Indice invertido\n",
    "\n",
    "## Integrantes\n",
    "- Nicho, Jorge\n",
    "- Riveros, Andres\n",
    "- Tanta, Johan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/johan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/johan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import math \n",
    "from decimal import Decimal\n",
    "import regex\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_stemmer(tweet_text):\n",
    "    tokens = nltk.word_tokenize(tweet_text)\n",
    "    stoplist = stopwords.words(\"spanish\")\n",
    "    stoplist += ['/…','--','RT','`','@','|','¿','?', '¡', '!','aqui', '.', ',', ';', '«', '»', ':', '(', ')', '\"','en', 'En', 'la', 'La', 'con', 'Con','sin', 'Sin', 'al', 'Al', 'de', 'De', 'el', 'El', '#', '$', '^', '&', '*', '%']\n",
    "    tokens_clean = tokens.copy()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in stoplist:\n",
    "            tokens_clean.remove(token)\n",
    "            \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    for i in range(len(tokens_clean)):\n",
    "        tokens_clean[i] = stemmer.stem(tokens_clean[i])\n",
    "        \n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_2018-09-28.json\n",
      "tweets_2018-10-04.json\n",
      "tweets_2018-09-06.json\n",
      "tweets_2018-09-16.json\n",
      "tweets_2018-08-31.json\n",
      "tweets_2018-08-24.json\n",
      "tweets_2018-08-13.json\n",
      "tweets_2018-10-01.json\n",
      "tweets_2018-09-09.json\n",
      "tweets_2018-08-12.json\n",
      "tweets_2018-09-11.json\n",
      "tweets_2018-09-08.json\n",
      "tweets_2018-09-29.json\n",
      "tweets_2018-09-10.json\n",
      "tweets_2018-09-07.json\n",
      "tweets_2018-10-03.json\n",
      "tweets_2018-08-30.json\n",
      "tweets_2018-09-19.json\n",
      "tweets_2018-08-19.json\n",
      "tweets_2018-09-04.json\n",
      "tweets_2018-09-15.json\n",
      "tweets_2018-09-24.json\n",
      "tweets_2018-10-02.json\n",
      "tweets_2018-09-17.json\n",
      "tweets_2018-08-25.json\n",
      "tweets_2018-08-09.json\n",
      "tweets_2018-08-20.json\n",
      "tweets_2018-09-25.json\n",
      "tweets_2018-08-21.json\n",
      "tweets_2018-08-18.json\n",
      "tweets_2018-09-27.json\n",
      "tweets_2018-10-05.json\n",
      "tweets_2018-08-16.json\n",
      "tweets_2018-09-14.json\n",
      "tweets_2018-09-03.json\n",
      "tweets_2018-08-26.json\n",
      "tweets_2018-08-07.json\n",
      "tweets_2018-09-21.json\n",
      "tweets_2018-08-22.json\n",
      "tweets_2018-09-05.json\n",
      "tweets_2018-08-14.json\n",
      "tweets_2018-08-10.json\n",
      "tweets_2018-09-12.json\n",
      "tweets_2018-08-08.json\n",
      "tweets_2018-08-28.json\n",
      "tweets_2018-09-18.json\n",
      "tweets_2018-09-22.json\n",
      "tweets_2018-08-15.json\n",
      "tweets_2018-09-20.json\n",
      "tweets_2018-08-17.json\n",
      "tweets_2018-09-26.json\n",
      "tweets_2018-08-23.json\n",
      "tweets_2018-09-23.json\n",
      "tweets_2018-08-29.json\n",
      "tweets_2018-09-30.json\n",
      "tweets_2018-09-13.json\n",
      "tweets_2018-08-27.json\n",
      "tweets_2018-09-02.json\n",
      "tweets_2018-08-11.json\n"
     ]
    }
   ],
   "source": [
    "json_path = './data/data_elecciones/'\n",
    "tweets_folder = os.listdir(json_path)\n",
    "inverted_index = {}\n",
    "term_termid_dict = {}\n",
    "filenameid_filename_dict = {}\n",
    "tweet_termids_dict = {}\n",
    "N = 0\n",
    "NTerms = 0\n",
    "for tweets_file in tweets_folder:\n",
    "    filenameid = len(filenameid_filename_dict)\n",
    "    filenameid_filename_dict[filenameid] = tweets_file\n",
    "    with open(json_path + tweets_file, encoding=\"utf8\") as file:\n",
    "        json_content = json.load(file)\n",
    "        for tweet in json_content:\n",
    "            N += 1\n",
    "            tokens_clean = stopwords_stemmer(tweet[\"text\"])\n",
    "            each_term_frequency = Counter(tokens_clean)\n",
    "            tweet_id = tweet[\"id\"]\n",
    "            norma = 0\n",
    "            tweet_termids_dict[tweet_id] = [filenameid, {}, norma]\n",
    "            for term in each_term_frequency:\n",
    "                freq = each_term_frequency[term]\n",
    "                if not (term in term_termid_dict):\n",
    "                    term_termid_dict[term] = len(term_termid_dict)\n",
    "                \n",
    "                term_id = term_termid_dict[term]\n",
    "                tweet_termids_dict[tweet_id][1][term_id] = 0.0\n",
    "\n",
    "                if not (term_id in inverted_index):\n",
    "                    inverted_index[term_id] = {tweet_id : freq}\n",
    "                else:\n",
    "                    inverted_index[term_id][tweet_id] = freq\n",
    "    print(tweets_file)\n",
    "\n",
    "termid_df_dict = {}\n",
    "for term_id in inverted_index:\n",
    "    posting_list = inverted_index[term_id]\n",
    "    termid_df_dict[term_id] = len(posting_list)\n",
    "NTerms = len(term_termid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightTF_IDF(posting_list, tweet_id):\n",
    "    tf = posting_list[tweet_id]\n",
    "    df = float(len(posting_list))\n",
    "    return math.log(1+tf, 10) * math.log(N / df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def setWeightTF_IDF():\n",
    "    for term_id in inverted_index:\n",
    "        posting_list = inverted_index[term_id]\n",
    "        for tweet_id in posting_list:\n",
    "            tweet_termids_dict[tweet_id][1][term_id] = getWeightTF_IDF(posting_list, tweet_id)\n",
    "\n",
    "def getNorma(posting_list):\n",
    "    values = np.array(list(posting_list.values()))\n",
    "    return np.linalg.norm(values)\n",
    "\n",
    "def setNorma():\n",
    "    for tweet_id in tweet_termids_dict:\n",
    "        posting_list = tweet_termids_dict[tweet_id][1]\n",
    "        tweet_termids_dict[tweet_id][2] = getNorma(posting_list)\n",
    "    \n",
    "setWeightTF_IDF()\n",
    "setNorma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "bin_path = './bin/'\n",
    "term_termid_dict_file = open(bin_path + \"term_termid_dict.dat\", \"wb\")\n",
    "inverted_index_file = open(bin_path + \"inverted_index.dat\", \"wb\")\n",
    "tweet_termids_dict_file = open(bin_path + \"tweet_termids_dict.dat\", \"wb\")\n",
    "filenameid_filename_dict_file = open(bin_path + \"filenameid_filename_dict.dat\", \"wb\")\n",
    "termid_df_dict_file = open(bin_path + \"termid_df_dict.dat\", \"wb\")\n",
    "N_file = open(bin_path + \"N.dat\", \"wb\")\n",
    "NTerms_file = open(bin_path + \"NTerms.dat\", \"wb\")\n",
    "\n",
    "pickle.dump(term_termid_dict, term_termid_dict_file)\n",
    "pickle.dump(inverted_index, inverted_index_file)\n",
    "pickle.dump(tweet_termids_dict, tweet_termids_dict_file)\n",
    "pickle.dump(filenameid_filename_dict, filenameid_filename_dict_file)\n",
    "pickle.dump(termid_df_dict, termid_df_dict_file)\n",
    "pickle.dump(N, N_file)\n",
    "pickle.dump(NTerms, NTerms_file)\n",
    "\n",
    "term_termid_dict_file.close()\n",
    "inverted_index_file.close()\n",
    "tweet_termids_dict_file.close()\n",
    "filenameid_filename_dict_file.close()\n",
    "N_file.close()\n",
    "NTerms_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de terminos:  128562\n",
      "Cantidad de Tweets:  479945\n"
     ]
    }
   ],
   "source": [
    "print(\"Cantidad de terminos: \", NTerms)\n",
    "print(\"Cantidad de Tweets: \", N)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}